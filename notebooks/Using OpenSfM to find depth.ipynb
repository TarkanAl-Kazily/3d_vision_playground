{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying OpenSfM to find the depth for points in an image\n",
    "\n",
    "OpenSfM is able to fully construct a point cloud for a collection of images that captures 3D information on the scene. We want to leverage this point cloud to create a 3D parametric representation. As such, we need to be able to go from detected wireframe features to 3D points in the point cloud.\n",
    "\n",
    "The general idea will be to attempt to project out from a detected line endpoint/junction in an image to an approximate location in the depth map found by OpenSfM. We can attempt to \"interpolate\" depth by using the depth values of points that project close to the given point, and assuming they should be coplanar or just averaging the depths.\n",
    "\n",
    "We should investigate how to access the resulting `merged.ply` point cloud data as well as the camera poses in order to accomplish the above.\n",
    "\n",
    "## Follow the resources in SfM.ipynb for setup and running the OpenSfM pipeline\n",
    "\n",
    "For this, using the OpenSfM `opensfm_run_all` executable should be good enough to generate your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import utils\n",
    "import numpy as np\n",
    "import yaml, json\n",
    "import cv2\n",
    "from plyfile import PlyData\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../OpenSfM')\n",
    "from opensfm import features, config\n",
    "from sfm import utils as sfm_util\n",
    "\n",
    "from scipy.spatial.transform import Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration parameters created using the steps in BasicOpenCV.ipynb\n",
    "# NOTE: Camera calibration parameters will not work with video frames!\n",
    "intrinsic_mat = np.load(utils.data(\"numpy/intrinsic_mat.npy\"))\n",
    "distortion_mat = np.load(utils.data(\"numpy/distortion_mat.npy\"))\n",
    "# Average the two focal lengths to get a best guess focal length\n",
    "f = (intrinsic_mat[0, 0] + intrinsic_mat[1, 1]) / (2.0 * 1920.0)\n",
    "# k1 and k2 are first two parameters of the distortion matrix\n",
    "k1 = distortion_mat[0, 0]\n",
    "k2 = distortion_mat[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{}\\n{}\\n{}\".format(f, k1, k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Set the project directory path here\n",
    "project_dir = utils.data(\"door_closed/\")\n",
    "\n",
    "\n",
    "conf = config.load_config(os.path.join(project_dir, \"config.yaml\"))\n",
    "depthmaps_dir = os.path.join(project_dir, \"undistorted/depthmaps\")\n",
    "with open(os.path.join(project_dir, \"reports/reconstruction.json\")) as f:\n",
    "    reconstruction_report = json.load(f)\n",
    "image_dir = os.path.join(project_dir, \"images\")\n",
    "\n",
    "# The merged.ply contains the depth information for points in the images (probably redundant with reconstruction.meshed.json)\n",
    "numpy_merged_points = os.path.join(project_dir, \"merged_points.npy\")\n",
    "try:\n",
    "    points = np.load(numpy_merged_points)\n",
    "except FileNotFoundError:\n",
    "    merged_ply = PlyData.read(os.path.join(depthmaps_dir, \"merged.ply\"))\n",
    "    element = merged_ply.elements[0]\n",
    "    points = np.vstack((element.data['x'], element.data['y'], element.data['z'])).transpose()\n",
    "    np.save(numpy_merged_points, points)\n",
    "    print(\"Created numpy file for merged.ply points\")\n",
    "\n",
    "\n",
    "# reconstruction.meshed.json contain the rotations and translations of each camera, along with the mesh points in the image\n",
    "with open(os.path.join(project_dir, \"reconstruction.meshed.json\")) as f:\n",
    "    reconstruction_meshed = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reconstruction_report[\"not_reconstructed_images\"])\n",
    "\n",
    "other_list = []\n",
    "\n",
    "for imname in os.listdir(image_dir):\n",
    "    print(\"Processing {}...\".format(imname))\n",
    "\n",
    "    prunedname = imname + \".pruned.npz\"\n",
    "    try:\n",
    "        pruned = np.load(os.path.join(depthmaps_dir, prunedname))\n",
    "    except FileNotFoundError:\n",
    "        print(\"Skipping {}: No depthmap found\".format(imname))\n",
    "        continue\n",
    "    #im = cv2.imread(os.path.join(image_dir, imnamejpg))\n",
    "    points = pruned[\"points\"]\n",
    "    if points.shape[0] == 0:\n",
    "        print(\"Skipping {}: No points in pruned mesh\".format(imname))\n",
    "        continue\n",
    "    other_list.append(imname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wireframe import Wireframe, WireframeGraph\n",
    "\n",
    "# Make sure to put your pretrained model data in the data directory!\n",
    "config_file = utils.data(\"wireframe.yaml\")\n",
    "model_file = utils.data(\"pretrained_lcnn.pth.tar\")\n",
    "\n",
    "w = Wireframe(config_file, model_file, \"\")\n",
    "\n",
    "if not w.setup():\n",
    "    print(\"An error occured trying to setup the wireframe: {}\".format(w.error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_K_dist(camera):\n",
    "    camera_name = next(iter(camera.keys()))\n",
    "    width = camera_params_dict[camera_name][\"width\"]\n",
    "    height = camera_params_dict[camera_name][\"height\"]\n",
    "    focal = camera_params_dict[camera_name][\"focal\"]\n",
    "    k1 = camera_params_dict[camera_name][\"k1\"]\n",
    "    k2 = camera_params_dict[camera_name][\"k2\"]\n",
    "\n",
    "    K = np.array([[focal * max(width, height), 0, 0.5 * (width - 1)],\n",
    "                  [0, focal * max(width, height), 0.5 * (height - 1)],\n",
    "                  [0, 0, 1]])\n",
    "    distortion = np.array([k1, k2, 0, 0, 0])\n",
    "    return K, distortion\n",
    "\n",
    "# Function to reproject a point in the point cloud to a point in the original image\n",
    "def project_point(points, R, T, K, distortion):\n",
    "    points, _ = cv2.projectPoints(points, R, T, K, distortion)\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_point_near_line(p, l, dist=20.0):\n",
    "    \"\"\"\n",
    "    Returns true if point is close to line and doesn't extend past the endpoints of the line.\n",
    "    \"\"\"\n",
    "    start = l[0, ::-1]\n",
    "    end = l[1, ::-1]\n",
    "    perp_dir = (np.array([[0, -1], [1, 0]]) @ (end - start)) / np.linalg.norm(end - start)\n",
    "    p = np.squeeze(p)\n",
    "    d = np.dot(perp_dir, p - start)\n",
    "    if np.abs(d) > dist:\n",
    "        return False\n",
    "    t = np.dot(end - start, p - start)\n",
    "    if t < 0 or t > np.linalg.norm(end - start) ** 2:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_points_to_ply(filepath, points, c=np.array([255, 255, 255]), r=50.0):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        print(\"ply\", file=f)\n",
    "        print(\"format ascii 1.0\", file=f)\n",
    "        print(\"element vertex {}\".format(len(points)), file=f)\n",
    "        print(\"property float x\", file=f)\n",
    "        print(\"property float y\", file=f)\n",
    "        print(\"property float z\", file=f)\n",
    "        print(\"property uchar red\", file=f)\n",
    "        print(\"property uchar green\", file=f)\n",
    "        print(\"property uchar blue\", file=f)\n",
    "        print(\"property float radius\", file=f)\n",
    "        print(\"end_header\", file=f)\n",
    "        for x, y, z in points:\n",
    "            print(\"{} {} {} {} {} {} {}\".format(x, y, z, c[0], c[1], c[2], r), file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(imname, info, camera, debug=True):\n",
    "    \"\"\"\n",
    "    Serves to run on the 3D information given by the reconstruction meshed json file.\n",
    "    \n",
    "    Arguments:\n",
    "    imname -- string image name to process (original file found in images dir)\n",
    "    info -- dictionary retrieved from reconstructed_meshed\n",
    "    \"\"\"\n",
    "    print(\"Processing {}...\".format(imname))\n",
    "    impath = str(os.path.join(image_dir, imname))\n",
    "    im = cv2.imread(impath)\n",
    "    points = np.array(info[\"vertices\"])\n",
    "    rotation = np.array(info[\"rotation\"])\n",
    "    translation = np.array(info[\"translation\"])\n",
    "    K, distortion = get_K_dist(camera)\n",
    "    if debug:\n",
    "        print(\"Rotation:\\n{}\\nTranslation:\\n{}\".format(rotation,translation))\n",
    "        print(\"Depth map consists of {} points\".format(points.shape[0]))\n",
    "    \n",
    "    impoints = project_point(points, rotation, translation, K, distortion)\n",
    "    if debug:\n",
    "        for p in impoints:\n",
    "            p = np.squeeze(p)\n",
    "            if p[0] < 0.0 or p[0] > width:\n",
    "                print(\"Width out of bounds: {}\".format(p))\n",
    "            elif p[1] < 0.0 or p[1] > height:\n",
    "                print(\"Height out of bounds: {}\".format(p))\n",
    "            else:\n",
    "                plt.scatter([p[0]], [p[1]])\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "\n",
    "    rec = w.parse(impath)\n",
    "    nlines, nscores = rec.postprocess(threshold=0.9)\n",
    "    if debug:\n",
    "        print(\"Wireframe found {} lines with score passing threshold\".format(nlines.shape[0]))\n",
    "        graph = WireframeGraph(rec, threshold=0.9)\n",
    "        graph.plot_graph(graph.g, im)\n",
    "    \n",
    "    for l_num, l in enumerate(nlines):\n",
    "        print(\"Working on line num {}\".format(l_num))\n",
    "        close_points = []\n",
    "        world_points = []\n",
    "        for i, p in enumerate(impoints):\n",
    "            p = np.squeeze(p)\n",
    "            if is_point_near_line(p, l, dist=20.0):\n",
    "                close_points.append(p)\n",
    "                world_points.append(points[i])\n",
    "        if debug:\n",
    "            plt.plot([l[0, 1], l[1, 1]], [l[0, 0], l[1, 0]], c='r')\n",
    "            for p in close_points:\n",
    "                plt.scatter([p[0]], [p[1]])\n",
    "            plt.imshow(im)\n",
    "            plt.show()\n",
    "        output_ply_file = imname + \".line_{}.ply\".format(l_num)\n",
    "        res_ply_dir = os.path.join(project_dir, \"my_ply/{}_lines/\".format(imname[:-3]))\n",
    "        os.makedirs(res_ply_dir, exist_ok=True)\n",
    "        write_points_to_ply(os.path.join(res_ply_dir, output_ply_file), world_points, c=np.array([255, 0, 0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = reconstruction_meshed[0]['cameras']\n",
    "for r in reconstruction_meshed:\n",
    "    for imname in r['shots'].keys():\n",
    "        process_image(imname, r['shots'][imname], r['cameras'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lcnn]",
   "language": "python",
   "name": "conda-env-lcnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
