{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying OpenSfM to find the depth for points in an image\n",
    "\n",
    "OpenSfM is able to fully construct a point cloud for a collection of images that captures 3D information on the scene. We want to leverage this point cloud to create a 3D parametric representation. As such, we need to be able to go from detected wireframe features to 3D points in the point cloud.\n",
    "\n",
    "The general idea will be to attempt to project out from a detected line endpoint/junction in an image to an approximate location in the depth map found by OpenSfM. We can attempt to \"interpolate\" depth by using the depth values of points that project close to the given point, and assuming they should be coplanar or just averaging the depths.\n",
    "\n",
    "We should investigate how to access the resulting `merged.ply` point cloud data as well as the camera poses in order to accomplish the above.\n",
    "\n",
    "## Follow the resources in SfM.ipynb for setup and running the OpenSfM pipeline\n",
    "\n",
    "For this, using the OpenSfM `opensfm_run_all` executable should be good enough to generate your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import utils\n",
    "import numpy as np\n",
    "import yaml, json\n",
    "import cv2\n",
    "from plyfile import PlyData\n",
    "\n",
    "sys.path.append('../OpenSfM')\n",
    "from opensfm import features, config\n",
    "from sfm import utils as sfm_util\n",
    "\n",
    "from scipy.spatial.transform import Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration parameters created using the steps in BasicOpenCV.ipynb\n",
    "# NOTE: Camera calibration parameters will not work with video frames!\n",
    "intrinsic_mat = np.load(utils.data(\"numpy/intrinsic_mat.npy\"))\n",
    "distortion_mat = np.load(utils.data(\"numpy/distortion_mat.npy\"))\n",
    "# Average the two focal lengths to get a best guess focal length\n",
    "f = (intrinsic_mat[0, 0] + intrinsic_mat[1, 1]) / 2.0\n",
    "# k1 and k2 are first two parameters of the distortion matrix\n",
    "k1 = distortion_mat[0, 0]\n",
    "k2 = distortion_mat[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3308.435718742333\n",
      "0.30446738772195164\n",
      "-2.1296322572508846\n"
     ]
    }
   ],
   "source": [
    "print(\"{}\\n{}\\n{}\".format(f, k1, k2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Set the project directory path here\n",
    "project_dir = utils.data(\"door_closed/\")\n",
    "\n",
    "\n",
    "conf = config.load_config(os.path.join(project_dir, \"config.yaml\"))\n",
    "depthmaps_dir = os.path.join(project_dir, \"undistorted/depthmaps\")\n",
    "with open(os.path.join(project_dir, \"reports/reconstruction.json\")) as f:\n",
    "    reconstruction_report = json.load(f)\n",
    "image_dir = os.path.join(project_dir, \"images\")\n",
    "\n",
    "# The merged.ply contains the depth information for points in the images (probably redundant with reconstruction.meshed.json)\n",
    "numpy_merged_points = os.path.join(project_dir, \"merged_points.npy\")\n",
    "try:\n",
    "    points = np.load(numpy_merged_points)\n",
    "except FileNotFoundError:\n",
    "    merged_ply = PlyData.read(os.path.join(depthmaps_dir, \"merged.ply\"))\n",
    "    element = merged_ply.elements[0]\n",
    "    points = np.vstack((element.data['x'], element.data['y'], element.data['z'])).transpose()\n",
    "    np.save(numpy_merged_points, points)\n",
    "    print(\"Created numpy file for merged.ply points\")\n",
    "\n",
    "\n",
    "# reconstruction.meshed.json contain the rotations and translations of each camera, along with the mesh points in the image\n",
    "with open(os.path.join(project_dir, \"reconstruction.meshed.json\")) as f:\n",
    "    reconstruction_meshed = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['img_25.png', 'img_43.png', 'img_59.png', 'img_53.png', 'img_52.png', 'img_46.png', 'img_24.png']\n",
      "Processing img_23.png...\n",
      "Skipping img_23.png: No points in pruned mesh\n",
      "Processing img_14.png...\n",
      "Processing img_31.png...\n",
      "Skipping img_31.png: No depthmap found\n",
      "Processing img_55.png...\n",
      "Skipping img_55.png: No depthmap found\n",
      "Processing img_44.png...\n",
      "Skipping img_44.png: No depthmap found\n",
      "Processing img_59.png...\n",
      "Skipping img_59.png: No depthmap found\n",
      "Processing img_63.png...\n",
      "Processing img_20.png...\n",
      "Processing img_13.png...\n",
      "Processing img_61.png...\n",
      "Processing img_52.png...\n",
      "Skipping img_52.png: No depthmap found\n",
      "Processing img_49.png...\n",
      "Skipping img_49.png: No depthmap found\n",
      "Processing img_60.png...\n",
      "Processing img_22.png...\n",
      "Processing img_57.png...\n",
      "Skipping img_57.png: No depthmap found\n",
      "Processing img_28.png...\n",
      "Skipping img_28.png: No depthmap found\n",
      "Processing img_12.png...\n",
      "Processing img_26.png...\n",
      "Skipping img_26.png: No depthmap found\n",
      "Processing img_54.png...\n",
      "Skipping img_54.png: No depthmap found\n",
      "Processing img_62.png...\n",
      "Processing img_36.png...\n",
      "Processing img_56.png...\n",
      "Skipping img_56.png: No depthmap found\n",
      "Processing img_47.png...\n",
      "Skipping img_47.png: No depthmap found\n",
      "Processing img_35.png...\n",
      "Processing img_4.png...\n",
      "Processing img_5.png...\n",
      "Processing img_32.png...\n",
      "Skipping img_32.png: No depthmap found\n",
      "Processing img_58.png...\n",
      "Skipping img_58.png: No depthmap found\n",
      "Processing img_41.png...\n",
      "Processing img_64.png...\n",
      "Processing img_0.png...\n",
      "Processing img_21.png...\n",
      "Processing img_38.png...\n",
      "Processing img_25.png...\n",
      "Skipping img_25.png: No depthmap found\n",
      "Processing img_29.png...\n",
      "Skipping img_29.png: No depthmap found\n",
      "Processing img_50.png...\n",
      "Skipping img_50.png: No depthmap found\n",
      "Processing img_27.png...\n",
      "Skipping img_27.png: No depthmap found\n",
      "Processing img_11.png...\n",
      "Processing img_6.png...\n",
      "Processing img_48.png...\n",
      "Skipping img_48.png: No points in pruned mesh\n",
      "Processing img_46.png...\n",
      "Skipping img_46.png: No depthmap found\n",
      "Processing img_37.png...\n",
      "Processing img_30.png...\n",
      "Skipping img_30.png: No depthmap found\n",
      "Processing img_45.png...\n",
      "Skipping img_45.png: No depthmap found\n",
      "Processing img_33.png...\n",
      "Skipping img_33.png: No depthmap found\n",
      "Processing img_3.png...\n",
      "Processing img_39.png...\n",
      "Skipping img_39.png: No depthmap found\n",
      "Processing img_18.png...\n",
      "Processing img_19.png...\n",
      "Processing img_16.png...\n",
      "Processing img_51.png...\n",
      "Skipping img_51.png: No depthmap found\n",
      "Processing img_43.png...\n",
      "Skipping img_43.png: No depthmap found\n",
      "Processing img_1.png...\n",
      "Processing img_9.png...\n",
      "Processing img_53.png...\n",
      "Skipping img_53.png: No depthmap found\n",
      "Processing img_17.png...\n",
      "Processing img_2.png...\n",
      "Processing img_34.png...\n",
      "Processing img_24.png...\n",
      "Skipping img_24.png: No depthmap found\n",
      "Processing img_7.png...\n",
      "Processing img_42.png...\n",
      "Processing img_15.png...\n",
      "Processing img_40.png...\n",
      "Skipping img_40.png: No points in pruned mesh\n",
      "Processing img_8.png...\n",
      "Processing img_10.png...\n"
     ]
    }
   ],
   "source": [
    "print(reconstruction_report[\"not_reconstructed_images\"])\n",
    "\n",
    "other_list = []\n",
    "\n",
    "for imname in os.listdir(image_dir):\n",
    "    print(\"Processing {}...\".format(imname))\n",
    "\n",
    "    prunedname = imname + \".pruned.npz\"\n",
    "    try:\n",
    "        pruned = np.load(os.path.join(depthmaps_dir, prunedname))\n",
    "    except FileNotFoundError:\n",
    "        print(\"Skipping {}: No depthmap found\".format(imname))\n",
    "        continue\n",
    "    #im = cv2.imread(os.path.join(image_dir, imnamejpg))\n",
    "    points = pruned[\"points\"]\n",
    "    if points.shape[0] == 0:\n",
    "        print(\"Skipping {}: No points in pruned mesh\".format(imname))\n",
    "        continue\n",
    "    other_list.append(imname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available\n"
     ]
    }
   ],
   "source": [
    "from wireframe import Wireframe\n",
    "\n",
    "# Make sure to put your pretrained model data in the data directory!\n",
    "config_file = utils.data(\"wireframe.yaml\")\n",
    "model_file = utils.data(\"pretrained_lcnn.pth.tar\")\n",
    "\n",
    "w = Wireframe(config_file, model_file, \"\")\n",
    "\n",
    "if not w.setup():\n",
    "    print(\"An error occured trying to setup the wireframe: {}\".format(w.error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(imname, info, camera, debug=True):\n",
    "    \"\"\"\n",
    "    Serves to run on the 3D information given by the reconstruction meshed json file.\n",
    "    \n",
    "    Arguments:\n",
    "    imname -- string image name to process (original file found in images dir)\n",
    "    info -- dictionary retrieved from reconstructed_meshed\n",
    "    \"\"\"\n",
    "    print(\"Processing {}...\".format(imname))\n",
    "    impath = str(os.path.join(image_dir, imname))\n",
    "    points = np.array(info[\"vertices\"])\n",
    "    rotation = Rotation.from_rotvec(info[\"rotation\"]).as_matrix()\n",
    "    translation = np.array(info[\"translation\"])\n",
    "    if debug:\n",
    "        print(\"Rotation:\\n{}\\nTranslation:\\n{}\".format(rotation,translation))\n",
    "        print(\"Depth map consists of {} points\".format(points.shape[0]))\n",
    "    \n",
    "    rec = w.parse(impath)\n",
    "    nlines, nscores = rec.postprocess(threshold=0.9)\n",
    "    if debug:\n",
    "        print(\"Wireframe found {} lines with score passing threshold\".format(nlines.shape[0]))\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'v2 unknown unknown 1920 1080 perspective 0': {'projection_type': 'perspective', 'width': 1920, 'height': 1080, 'focal': 0.9117969061343857, 'k1': -0.00010384566236784214, 'k2': -0.037270710547293515}}\n",
      "{'v2 unknown unknown 1920 1080 perspective 0': {'projection_type': 'perspective', 'width': 1920, 'height': 1080, 'focal': 0.8685991044967001, 'k1': -0.024741197839431205, 'k2': -0.01730096812568517}}\n",
      "{'v2 unknown unknown 1920 1080 perspective 0': {'projection_type': 'perspective', 'width': 1920, 'height': 1080, 'focal': 0.8498940797661843, 'k1': 0.0003137101231893825, 'k2': -0.0026578494644543685}}\n"
     ]
    }
   ],
   "source": [
    "camera = reconstruction_meshed[0]['cameras']\n",
    "for r in reconstruction_meshed:\n",
    "    print(r['cameras'])\n",
    "    continue\n",
    "    for imname in r['shots'].keys():\n",
    "        process_image(imname, r['shots'][imname], camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lcnn]",
   "language": "python",
   "name": "conda-env-lcnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
